{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-29T03:13:54.940792Z","iopub.execute_input":"2022-11-29T03:13:54.941278Z","iopub.status.idle":"2022-11-29T03:13:55.001913Z","shell.execute_reply.started":"2022-11-29T03:13:54.941188Z","shell.execute_reply":"2022-11-29T03:13:55.000966Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/datacon-22/Processed_data/bookings.csv\n/kaggle/input/datacon-22/Processed_data/hotels_data.csv\n/kaggle/input/datacon-22/Processed_data/bookings_data.csv\n/kaggle/input/datacon-22/Processed_data/train_data.csv\n/kaggle/input/datacon-22/Processed_data/customer_data.csv\n/kaggle/input/datacon-22/Processed_data/sample_submission_5.csv\n/kaggle/input/datacon-22/Processed_data/payments_data.csv\n/kaggle/input/27-11-filtering-columns-date-new/__results__.html\n/kaggle/input/27-11-filtering-columns-date-new/__notebook__.ipynb\n/kaggle/input/27-11-filtering-columns-date-new/__output__.json\n/kaggle/input/27-11-filtering-columns-date-new/custom.css\n/kaggle/input/27-11-filtering-columns-date-new/full_data_to_use_27-11.csv\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___80_0.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___50_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___9_0.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___38_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___121_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___104_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___84_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___54_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___61_0.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___14_0.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___70_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___77_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___110_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___62_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___113_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___86_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___49_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___73_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___48_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___43_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___69_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___112_2.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___16_0.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___65_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___82_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___59_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___46_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___91_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___74_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___88_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___107_1.png\n/kaggle/input/27-11-filtering-columns-date-new/__results___files/__results___101_1.png\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-11-29T03:13:55.003324Z","iopub.execute_input":"2022-11-29T03:13:55.003650Z","iopub.status.idle":"2022-11-29T03:13:55.009033Z","shell.execute_reply.started":"2022-11-29T03:13:55.003623Z","shell.execute_reply":"2022-11-29T03:13:55.007695Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv('/kaggle/input/27-11-filtering-columns-date-new/full_data_to_use_27-11.csv')\n\ndf_training = df1.loc[df1['set'] == 'training']\ndf_testing = df1.loc[df1['set'] == 'testing']\n\ndf_training.drop([\"booking_id\", 'set'], axis=1, inplace = True)\ndf_testing.drop([\"booking_id\", 'set'], axis=1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T03:14:03.099706Z","iopub.execute_input":"2022-11-29T03:14:03.100074Z","iopub.status.idle":"2022-11-29T03:14:03.465037Z","shell.execute_reply.started":"2022-11-29T03:14:03.100045Z","shell.execute_reply":"2022-11-29T03:14:03.463380Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  errors=errors,\n","output_type":"stream"}]},{"cell_type":"code","source":"import optuna\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-11-29T03:21:31.334240Z","iopub.execute_input":"2022-11-29T03:21:31.334599Z","iopub.status.idle":"2022-11-29T03:21:33.344460Z","shell.execute_reply.started":"2022-11-29T03:21:31.334573Z","shell.execute_reply":"2022-11-29T03:21:33.343227Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Build a model by implementing define-by-run design from Optuna\ndef build_model_custom(trial):\n    \n    n_layers = trial.suggest_int(\"n_layers\", 1, 8)\n    layers = []\n    in_features = df_training.shape[1]\n    \n    for i in range(n_layers):\n        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 128)\n        layers.append(nn.Linear(in_features, out_features))\n        layers.append(nn.LeakyReLU())\n        p = trial.suggest_uniform(\"dropout_l{}\".format(i), 0.1, 0.5)\n        layers.append(nn.Dropout(p))\n        \n        in_features = out_features\n        \n    layers.append(nn.Linear(in_features, 5))\n    layers.append(nn.LeakyReLU())\n    return nn.Sequential(*layers)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate the accuracy of neural network with the addition of pruning mechanism\ndef train_and_evaluate(param, model, trial):\n    \n#     df = pd.read_csv('heart.csv')\n#     df = pd.get_dummies(df)\n    df = df_training.copy()\n    \n    train_data, val_data = train_test_split(df, test_size = 0.2, random_state = 42)\n    train, val = Dataset(train_data), Dataset(val_data)\n\n    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    criterion = torch.nn.functional.mse_loss()\n    \n    optimizer = getattr(optim, param['optimizer'])(model.parameters(), lr= param['learning_rate'])\n\n    if use_cuda:\n        model = model.cuda()\n        criterion = criterion.cuda()\n\n    for epoch_num in range(EPOCHS):\n        total_acc_train = 0\n        total_loss_train = 0\n\n        for train_input, train_label in train_dataloader:\n            \n            train_label = train_label.to(device)\n            train_input = train_input.to(device)\n\n            output = model(train_input.float())\n\n            batch_loss = criterion(output, train_label.long())\n            total_loss_train += batch_loss.item()\n\n            acc = (output.argmax(dim=1) == train_label).sum().item()\n            total_acc_train += acc\n\n            model.zero_grad()\n            batch_loss.backward()\n            optimizer.step()\n\n        total_acc_val = 0\n        total_loss_val = 0\n\n        with torch.no_grad():\n\n            for val_input, val_label in val_dataloader:\n\n                val_label = val_label.to(device)\n                val_input = val_input.to(device)\n\n                output = model(val_input.float())\n\n                batch_loss = criterion(output, val_label.long())\n                total_loss_val += batch_loss.item()\n\n                acc = (output.argmax(dim=1) == val_label).sum().item()\n                total_acc_val += acc\n\n        accuracy = total_acc_val/len(val_data)\n\n        # Add prune mechanism\n        trial.report(accuracy, epoch_num)\n\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n\n    return accuracy\n  \n# Define a set of hyperparameter values, build the model, train the model, and evaluate the accuracy\ndef objective(trial):\n\n     params = {\n              'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n              'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]),\n              }\n    \n     model = build_model_custom(trial)\n\n     accuracy = train_and_evaluate(params, model, trial)\n\n     return accuracy\n  \n  \nEPOCHS = 30\n    \nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\nstudy.optimize(objective, n_trials=30)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}